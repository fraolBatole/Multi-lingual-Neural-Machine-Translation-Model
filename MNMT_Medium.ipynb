{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/fraolBatole/Multi-lingual-Neural-Machine-Translation-Model/blob/main/MNMT_Medium.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Import all the required libraries."
      ],
      "metadata": {
        "id": "UD-ZWQy0XA3M"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CB3hzllVhKVC"
      },
      "outputs": [],
      "source": [
        "import csv\n",
        "import os\n",
        "import numpy as np\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "from keras.utils.np_utils import to_categorical\n",
        "from keras_preprocessing.sequence import pad_sequences\n",
        "import tensorflow_datasets as tfds\n",
        "from nltk.translate.bleu_score import corpus_bleu\n",
        "import os\n",
        "from keras.callbacks import ReduceLROnPlateau\n",
        "from keras.callbacks import ModelCheckpoint\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "import random\n",
        "import string\n",
        "import re"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Load the datasets. In this example, we used the Tatoeba dataset to show the basics."
      ],
      "metadata": {
        "id": "Nhz1x9LyWyBS"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OdmG4c70hxQs"
      },
      "outputs": [],
      "source": [
        "def load_data(path):\n",
        "    \"\"\"\n",
        "    Load dataset\n",
        "    \"\"\"\n",
        "    input_file = os.path.join(path)\n",
        "    with open(input_file, \"r\") as f:\n",
        "        data = f.read()\n",
        "\n",
        "    return data.split('\\n')\n",
        "\n",
        "def load_glob_embedding(num_words, embed_size=100, word_index=None):\n",
        "    from numpy import asarray\n",
        "    from numpy import zeros\n",
        "\n",
        "    embeddings_dictionary = dict()\n",
        "    glove_file = open('/content/drive/MyDrive/NMT/glove.6B.'+str(embed_size)+'d.txt', encoding=\"utf8\")\n",
        "\n",
        "    for line in glove_file:\n",
        "        records = line.split()\n",
        "        word = records[0]\n",
        "        vector_dimensions = asarray(records[1:], dtype='float32')\n",
        "        embeddings_dictionary[word] = vector_dimensions\n",
        "    glove_file.close()\n",
        "\n",
        "    embedding_matrix = zeros((num_words, embed_size))\n",
        "    for index, word in enumerate(word_index):\n",
        "        embedding_vector = embeddings_dictionary.get(word)\n",
        "        if embedding_vector is not None:\n",
        "            embedding_matrix[index] = embedding_vector\n",
        "\n",
        "    return embedding_matrix\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oTEQAFkOK8B4"
      },
      "outputs": [],
      "source": [
        "english_sentences = load_data('/content/drive/MyDrive/NMT/en_train.txt')\n",
        "french_sentences = load_data('/content/drive/MyDrive/NMT/fr_train.txt')\n",
        "german_sentences = load_data('/content/drive/MyDrive/NMT/de_train.txt')\n",
        "italian_sentences = load_data('/content/drive/MyDrive/NMT/it_train.txt')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jNH6q4fk63Uc"
      },
      "source": [
        "## Prepare the input dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b4BL1OgOiVFF"
      },
      "outputs": [],
      "source": [
        "text_pairs = []\n",
        "for english,french,german,italian in zip(english_sentences, french_sentences,german_sentences,italian_sentences):\n",
        "    english = \"[starten] \" + english + \" [enden]\"\n",
        "    french = \"[startfr] \" + french + \" [endfr]\"\n",
        "    german = \"[startde] \" + german + \" [endde]\"\n",
        "    italian = \"[startit] \" + italian + \" [endit]\"\n",
        "\n",
        "    text_pairs.append((english, french))\n",
        "    text_pairs.append((english, german))\n",
        "    text_pairs.append((english, italian))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w8044H00hfK-"
      },
      "source": [
        "# Building the model"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## Here, we define the constants \n",
        "\n",
        "embed_dim = 200\n",
        "latent_dim = 1024\n",
        "vocab_size = 30000\n",
        "sequence_length = 20\n",
        "batch_size = 64"
      ],
      "metadata": {
        "id": "LHp5hz8pXWmI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mrG_3rC8h01E",
        "outputId": "4e2ea5f3-444b-4386-d326-2c26d849f1cb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "('Do you like peanut butter?', '[startde] Magst du Erdnussbutter? [endde]')\n"
          ]
        }
      ],
      "source": [
        "print(random.choice(text_pairs))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "i4bKiSxSLKMI"
      },
      "outputs": [],
      "source": [
        "random.shuffle(text_pairs)\n",
        "\n",
        "num_val_samples = int(0.15 * len(text_pairs))\n",
        "num_train_samples = len(text_pairs) - 2 * num_val_samples\n",
        "train_pairs = text_pairs[:num_train_samples]\n",
        "val_pairs = text_pairs[num_train_samples:num_train_samples + num_val_samples]\n",
        "test_pairs = text_pairs[num_train_samples + num_val_samples:]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CB-X3EfULMec"
      },
      "outputs": [],
      "source": [
        "## Preprocess the dataset to remove unneccessary tokens.\n",
        "\n",
        "strip_chars = string.punctuation + \"Â¿\"\n",
        "strip_chars = strip_chars.replace(\"[\", \"\")\n",
        "strip_chars = strip_chars.replace(\"]\", \"\")\n",
        "\n",
        "def custom_standardization(input_string):\n",
        "    lowercase = tf.strings.lower(input_string)\n",
        "    return tf.strings.regex_replace(\n",
        "        lowercase, f\"[{re.escape(strip_chars)}]\", \"\")\n",
        "\n",
        "source_vectorization = layers.TextVectorization(\n",
        "    max_tokens=vocab_size,\n",
        "    output_mode=\"int\",\n",
        "    output_sequence_length=sequence_length,\n",
        ")\n",
        "\n",
        "target_vectorization = layers.TextVectorization(\n",
        "    max_tokens=vocab_size,\n",
        "    output_mode=\"int\",\n",
        "    output_sequence_length=sequence_length + 1,\n",
        "    standardize=custom_standardization,\n",
        ")\n",
        "\n",
        "train_source_texts = [pair[0] for pair in train_pairs]\n",
        "train_target_texts = [pair[1] for pair in train_pairs]\n",
        "source_vectorization.adapt(train_source_texts)\n",
        "target_vectorization.adapt(train_target_texts)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IhrYWMyHLPfr"
      },
      "outputs": [],
      "source": [
        "def format_dataset(eng, spa):\n",
        "    eng = source_vectorization(eng)\n",
        "    spa = target_vectorization(spa)\n",
        "    return ({\n",
        "        \"source\": eng,\n",
        "        \"target\": spa[:, :-1],\n",
        "    }, spa[:, 1:])\n",
        "\n",
        "def make_dataset(pairs):\n",
        "    eng_texts, spa_texts = zip(*pairs)\n",
        "    eng_texts = list(eng_texts)\n",
        "    spa_texts = list(spa_texts)\n",
        "    dataset = tf.data.Dataset.from_tensor_slices((eng_texts, spa_texts))\n",
        "    dataset = dataset.batch(batch_size)\n",
        "    dataset = dataset.map(format_dataset, num_parallel_calls=4)\n",
        "    return dataset.shuffle(2048).prefetch(16).cache()\n",
        "\n",
        "train_ds = make_dataset(train_pairs)\n",
        "val_ds = make_dataset(val_pairs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tFFNByKXLRtV",
        "outputId": "c11030c5-1d46-45b0-e2f1-ed45f27eb087"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "inputs['source'].shape: (64, 20)\n",
            "inputs['target'].shape: (64, 20)\n",
            "targets.shape: (64, 20)\n"
          ]
        }
      ],
      "source": [
        "for inputs, targets in train_ds.take(1):\n",
        "    print(f\"inputs['source'].shape: {inputs['source'].shape}\")\n",
        "    print(f\"inputs['target'].shape: {inputs['target'].shape}\")\n",
        "    print(f\"targets.shape: {targets.shape}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HUxTSb5ELV_S"
      },
      "outputs": [],
      "source": [
        "embedding_matrix = load_glob_embedding(vocab_size, 200, target_vectorization.get_vocabulary())\n",
        "\n",
        "source = keras.Input(shape=(None,), dtype=\"int64\", name=\"source\")\n",
        "\n",
        "x = layers.Embedding(vocab_size, embed_dim, weights=[embedding_matrix], mask_zero=True,\n",
        "                      name='embed_encoder', trainable=False)(source)\n",
        "\n",
        "encoded_source = layers.SimpleLSTM(latent_dim, return_sequences=True, activation='relu', name='lstm_encoder1')(x)\n",
        "encoded_source = layers.SimpleLSTM(latent_dim, return_sequences=True, activation='relu', name='lstm_encoder2')(encoded_source)\n",
        "encoded_source = layers.SimpleLSTM(latent_dim, return_sequences=True, activation='relu', name='lstm_encoder3')(encoded_source)\n",
        "encoded_state = layers.SimpleLSTM(latent_dim, activation='relu', name='rnn_encoder4')(encoded_source)\n",
        "\n",
        "past_target = keras.Input(shape=(None,), dtype=\"int64\", name=\"target\")\n",
        "x = layers.Embedding(vocab_size, embed_dim, mask_zero=True, name='embed_decoder')(past_target)\n",
        "\n",
        "decoder_rnn = layers.SimpleLSTM(latent_dim, return_sequences=True, activation='relu', name='lstm_decoder1')\n",
        "x = decoder_rnn(x, initial_state=encoder_states)\n",
        "x = layers.SimpleLSTM(latent_dim, return_sequences=True, activation='relu', name='lstm_decoder2')(x)\n",
        "x = layers.SimpleLSTM(latent_dim, return_sequences=True, activation='relu', name='lstm_decoder3')(x)\n",
        "x = layers.SimpleLSTM(latent_dim, return_sequences=True, activation='relu', name='lstm_decoder4')(x)\n",
        "\n",
        "x = layers.Dropout(0.5)(x)\n",
        "\n",
        "target_next_step = layers.TimeDistributed(layers.Dense(vocab_size, activation=\"softmax\", name='output'))(x)\n",
        "\n",
        "seq2seq_rnn = keras.Model([source, past_target], target_next_step)\n",
        "\n",
        "seq2seq_rnn.compile(\n",
        "    optimizer=\"rmsprop\",\n",
        "    loss=\"sparse_categorical_crossentropy\",\n",
        "    metrics=[\"accuracy\"])\n",
        "\n",
        "seq2seq_rnn.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "twWGTTxHLX0-"
      },
      "outputs": [],
      "source": [
        "seq2seq_rnn.fit(train_ds, epochs=3, validation_data=val_ds)\n",
        "\n",
        "seq2seq_rnn.save('MNMT.h5')"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "machine_shape": "hm",
      "provenance": [],
      "include_colab_link": true
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}